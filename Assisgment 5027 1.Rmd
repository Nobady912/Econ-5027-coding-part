---
title: "The Assisgnment one"
author: "Tie Ma"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

#Tie Ma
#student number: 101316917
#ECON 5027
#Assignment one 

```{r eval=FALSE, include=FALSE}
#just to make sure I have correct verson of file 
#202402115: Finished polish

```


```{r include=FALSE}
#################################################################################################################
#Tie Ma
#student number: 101316917
#ECON 5027
#Assignment one 
#################################################################################################################
#Clean the environment
rm(list = ls())

#set the direction
setwd("~/Documents/GitHub/Econ-5027-coding-part")

#Lode the all package that necessary.
#some of the packages may not been used in this homework.
#I just copy it around
library("MASS")
library("ggplot2")
library("sandwich")
##############################


```

Q2-D
```{r Q2-D-1, echo=FALSE}
#loding the data
Q2D_matrix <- matrix(c(
  0.23, -0.39, -0.03, 0.29, 0.33, -0.37, -0.32, -0.04, -0.51, -0.96,  # Yi
  1, 0, 1, 0, 0, 1, 0, 0, 1, 1,                                       # Xi1
  1, 2, 1, 1, 2, 2, 3, 3, 2, 3                                        # Xi2
),
nrow=10,  
ncol=3, 
byrow=FALSE )

#add one for the intercept
x <- cbind(1, Q2D_matrix[, 2:3]) 
y <- Q2D_matrix[, 1]

#time for the beta_hat
beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)

#constructed the model
Q2D_model <- function(x1, x2, beta_hat) {
  beta_hat[1] + beta_hat[2] * x1 + beta_hat[3] * x2 
}
#lode the data
Xi1_values <- c(0, 1)
Xi2_values <- c(1, 2, 3)

#print the value
rownames(beta_hat) <- c("beta_one_hat", "beta_two_hat", "beta_three_hat")
colnames(beta_hat) <- c("beta_hat")
print(beta_hat)


```

```{r Q-2-D-2, echo=FALSE}
#The data frame
Q2D_data_frame <- expand.grid(Xi1 = Xi1_values, Xi2 = Xi2_values)


# Calculate the conditional expectations everything
conditional_expectations <- c(
  beta_hat[1] + beta_hat[2] * 0 + beta_hat[3] * 1,
  beta_hat[1] + beta_hat[2] * 0 + beta_hat[3] * 2,
  beta_hat[1] + beta_hat[2] * 0 + beta_hat[3] * 3,
  beta_hat[1] + beta_hat[2] * 1 + beta_hat[3] * 1,
  beta_hat[1] + beta_hat[2] * 1 + beta_hat[3] * 2,
  beta_hat[1] + beta_hat[2] * 1 + beta_hat[3] * 3
)

#final output matrix
conditional_matrix <- cbind(
  "E[Yi|Xi1=x1,Xi2=x2]" = conditional_expectations,
  Xi1 = c(0, 0, 0, 1, 1, 1),
  Xi2 = c(1, 2, 3, 1, 2, 3)
)

# print time
print(conditional_matrix)
```

Q2-E
```{R Q2-E , echo=FALSE}
#calculate form Q2-a
#E[Yi|Xi1=0,Xi2=1]
E01 <- 0.29

#E[Yi|Xi1=0,Xi2=2]
E02 <- (-0.39+0.33)/2

#E[Yi|Xi1=0,Xi2=3]
E03 <- (-0.32-0.04)/2

#E[Yi|Xi1=0,Xi2=1]
E11 <-(0.23 - 0.03)/2

#E[Yi|Xi1=1,Xi2=2]
E12 <- (-0.37 -0.51)/2

#E[Yi|Xi1=1,Xi2=3]
E13 <- -0.96

QA_answer <-c(E01, E02, E03, E11, E12, E13)
#The comparsing Matrix

comparsing_matrix <- cbind(
  "E[Yi|Xi1=x1,Xi2=x2]_QD" = conditional_expectations,
  "E[Yi|Xi1=x1,Xi2=x2]_QA" = QA_answer,
  "difference(QD -QA)" = conditional_expectations-QA_answer,
  Xi1 = c(0, 0, 0, 1, 1, 1),
  Xi2 = c(1, 2, 3, 1, 2, 3)
)

print(comparsing_matrix )
```
This table displays the differences between the sample analogs of the conditional expectations we calculated manually in Question A and those estimated using OLS. The table suggests that there are differences between the two, but with a minor margin. Therefore, I think that model (0.1) is a plausible and correct representation.
  

Q3-A
```{r Q3-A , echo=FALSE }
#clean the environment for the the assignment. 
rm(list = ls())

#set the see
set.seed(101310927)

#set the variance matrix
evil_variance_matrix <- matrix(c(1.0964, -0.5313, -0.5730, 
                                 -0.5313, 0.9381, -0.4184,
                                 -0.5730, -0.4184, 1.0228), nrow = 3, ncol = 3, byrow = TRUE)

#set up the mu
mu <- c(1.1141, -0.6768, 3.3521)

# c sequence (from -1 to 1 by 0.1)
c_value <- seq(from = -1, to = 1, by = 0.1)

# Number of simulation rounds
R <- 1000

# Parameters 
# the n is for the data generate process within the loop
# the n_value of for the loop to keep going.
n_value <- c(100, 250, 500, 1000)

#set up the beta_ture
beta_true <- c(-0.8, 0, 0, 0.1) 

#the t-test with 0.05 degree of freedom (do not change to a you will lost it)
alpha <- 0.05

# The storage matrix
  #final_matrix
    Final_matrix <- matrix(0,ncol = length(c_value), nrow = length(n_value))

#the simulation time


for (i in 1:length(n_value)){
  #rest the n
  n <- n_value[i]
  #reset the TF matrix
  TF_matrix <- matrix(NA, ncol=length(c_value), nrow = R)
  
  #this loop run 1000 times with current value of n
  for (j in 1:R) {
    
    x <- mvrnorm(n, mu = mu, Sigma = evil_variance_matrix) # its should be "Sigma" not "sigma"
    
    # Add intercept (we now turn the x from 3x3 matrix to 3x4 matrix)
    x <- cbind(rep(1, n), x)
    # e need to be processed within the loop 
    e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
    #now its the time for the calculate the y!
    y <- x %*% beta_true + e
    
    # OLS estimation
    beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)
    eps_hat <- y - x %*% beta_hat
    
    #the degree of freedom 
    dof <- n - length(beta_hat)
    # the mu
    sigma_hat_sq <- (1 / dof) * sum(eps_hat^2)
    #estimate of the 
    v_beta_hat <- solve(t(x) %*% x) * sigma_hat_sq
    # standard error time!
    diag_v_beta_hat <- diag(v_beta_hat)
    sd <- sqrt(diag_v_beta_hat)
    
    #the T- test time!
    for (k in 1:length(c_value)) {
      #T statistic for the beta3
      t_st <- (beta_hat[3,] - c_value[k])/sd[3]
      #T test P value 
      t_p_value <- 1 - pt(abs(t_st), dof) #using the TA's note!
   #the matrix time!
      #count the p_value
      TF_matrix[j, k] <- t_p_value < 0.05
      #this things was crashed my mac for times
      #I spend 2 hours on this line a lone
     }
   }
  Final_matrix[i, ] <- colMeans(TF_matrix)
  #and other hour on this line.
 }

#god tis finally end, I rest in peace.

    
#change the row name of the matrix 
rownames(Final_matrix) <- c("n = 100", "n = 250", "n = 500", "n = 1000")
    

# Assuming Final_matrix and c_value are already defined
    
# Colors
dw_blue <- "#003b6f"  # Doctor Who blue
cr_red <- "#e91c25"   # Carlton red
ua_green <- "#007c41"  # ualberta green 
ua_gold <- "#ffdb05"  # ualberta gold
    
# Prepare the data
n_labels <- c("n = 100", "n = 250", "n = 500", "n = 1000")
colors <- c(dw_blue, cr_red, ua_green, ua_gold)
    
# Plotting
 plot(c_value, Final_matrix[1, ], type = "l", col = colors[1], ylim = c(min(Final_matrix), max(Final_matrix)), xlab = "Value of c", ylab = "Proportion of Rejections", main = "3A-Power Curves for Different Sample Sizes", lwd = 2, xlim = c(-1, 1))
lines(c_value, Final_matrix[2, ], col = colors[2], lwd = 2)
lines(c_value, Final_matrix[3, ], col = colors[3], lwd = 2)
lines(c_value, Final_matrix[4, ], col = colors[4], lwd = 2)
legend("top", legend = n_labels, col = colors, lty = 1, lwd = 2)

```

This graph shows the proportions of rejections for values of \( c \) moving closer to the true value of \( \\beta_2 = 0 \) across four different sample sizes (\( n \)). We can observe that as \( n \) increases, the proportion of times that the null hypothesis is rejected also increases for different values of \( c \). This may suggests that large number results in a lower chance of making a Type I error, as the law the big number function. The proportion rejected is not zero for any of the four values of \( n \), according to the graph. This indicates that even when the null hypothesis is true, there is always a non-zero chance of rejection due to random sampling variation, which is a Type I error.


Q3-B
```{r, echo=FALSE }
#clean the environment for the the assignment. 
rm(list = ls())
set.seed(101310927)
   
#set the variance matrix
evil_variance_matrix <- matrix(c(1.0964, -0.5313, -0.5730, 
                                     -0.5313, 0.9381, -0.4184,
                                     -0.5730, -0.4184, 1.0228), nrow = 3, ncol = 3, byrow = TRUE)

n_value <- c(100, 250, 500, 1000)
#set up the mu
mu <- c(1.1141, -0.6768, 3.3521)
    
# c sequence (from -1 to 1 by 0.1)
c_value <- seq(from = -1, to = 1, by = 0.1)
    
# Number of simulation rounds
R <- 1000
    
# Parameters 
#the n is for the data generate process within the loop
# the n_value of for the loop to keep going.
#n_value <- c(100, 250, 500, 1000)

#the t-test with 0.05 degree of freedom (do not change to a you will lost it)
alpha <- 0.05
    
#final_matrix
Final_matrix <- matrix(0,ncol = length(c_value), nrow = length(n_value))
    
#the simulation time
    for (i in 1:length(n_value)){
      #rest the n
      n <- n_value[i]
      #reset the TF matrix
      TF_matrix <- matrix(NA, ncol=length(c_value), nrow = R)
      
      #this loop run 1000 times with current value of n
      for (j in 1:R) {
        
        #take the section of the data from the DGP and to do the next set.
        #The following code are specific for the model 0.4.
        
        simulated_data <- mvrnorm(n, mu, evil_variance_matrix)
        #note: here the data does not have the intercept
        # beta1 + beta 2 + beta 3
        
        #take the beta1, beta2 and beta3 out and repute them together. 
        #and I can repose it for the rest of the question 3!
        #I am so smart!
    
        beta_one <- simulated_data[, 1]
        beta_two <- simulated_data[, 2]
        beta_three <- simulated_data[, 3]
      
        #the model 0.4 have intercept, beta_2 and beta_3
        x <- cbind(rep(1, n), simulated_data[, 2], simulated_data[, 3])
        
        # e need to be processed within the loop 
        p <- 3
        
        e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
       #e_token <- e[c(1, 3, 4)] #for the intercept, beta 2 and beta4 
        
        #now its the time for the calculate the y!
        beta_very_true <- c(-0.8, 0, 0.1)  # As a vector 
        #I spend two hour for make it look smarter but 
        #it does not work.
        
        
        y <- x %*% beta_very_true + e
        
        # OLS estimation
        beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)
        eps_hat <- y - x %*% beta_hat
        
        #the degree of freedom 
        dof <- n - length(beta_hat)
        # the mu
        sigma_hat_sq <- (1 / dof) * sum(eps_hat^2)
        #estimate of the 
        v_beta_hat <- solve(t(x) %*% x) * sigma_hat_sq
        # standard error time!
        diag_v_beta_hat <- diag(v_beta_hat)
        sd <- sqrt(diag_v_beta_hat)
        
        #the T- test time!
        for (k in 1:length(c_value)) {
          #T statistic for the beta2
          t_st <- (beta_hat[2,] - c_value[k])/sd[2]
          #T test P value 
          t_p_value <- 1 - pt(abs(t_st), dof) #using the TA's note!
          #the matrix time!
          #count the p_value
          TF_matrix[j, k] <- t_p_value < 0.05
          #this things was crashed my mac for times
          #I spend 2 hours on this line a lone
        }
      }
      Final_matrix[i, ] <- colMeans(TF_matrix)
      #and other hour on this line.
    }
    
#change the row name of the matrix 
rownames(Final_matrix) <- c("n = 100", "n = 250", "n = 500", "n = 1000")
    
# Assuming Final_matrix and c_value are already defined
    
# Colors
dw_blue <- "#003b6f"  # Doctor Who blue
cr_red <- "#e91c25"   # Carlton red
cr_black <- "#00000D" # Carleton black (AKA:black)
ua_green <- "#007c41"  # Ualberta green 
ua_gold <- "#ffdb05"  # Ualberta gold
    
# Prepare the data
n_labels <- c("n = 100", "n = 250", "n = 500", "n = 1000")
colors <- c(dw_blue, cr_red, ua_green, ua_gold)
    
# Plotting
plot(c_value, Final_matrix[1, ], type = "l", col = colors[1], ylim = c(min(Final_matrix), max(Final_matrix)), xlab = "Value of c", ylab = "proportion of rejections", main = "3-B power curves for different sample size", lwd = 2, xlim = c(-1, 1))
lines(c_value, Final_matrix[2, ], col = colors[2], lwd = 2)
lines(c_value, Final_matrix[3, ], col = colors[3], lwd = 2)
lines(c_value, Final_matrix[4, ], col = colors[4], lwd = 2)
legend("bottomleft", legend = n_labels, col = colors, lty = 1, lwd = 2)
    
```

  
  After removing $\beta_{1}$, where the true $\beta_{1}$ is equal to zero, we observed a significant drop in the proportion of Type I errors, which represent false positives. Its also show that while \( n \) increase, the type I errors has been droped, suggest the law of big number In the graph for part (a), the proportion of rejections exhibits a smooth decrease as the $c$-value approaches the true $\hat{\beta}_{2}$, indicating a gradual reduction in Type I errors. However, in comparison to part (b), Type I errors occur only when the $c$-value is close to 0.5 for $n = 100$, in contrast to starting at one in the graph for part (a) when $n = 100$. this happened because, including unnecessary predictors in a statistical model increases the risk of Type I and Type II errors due to an increase in model complexity.


Q3-C
```{r Q3-C , echo=FALSE }
#clean the environment for the the assignment. 
rm(list = ls())
set.seed(101310927)
    
#set the variance matrix
evil_variance_matrix <- matrix(c(1.0964, -0.5313, -0.5730, 
                                     -0.5313, 0.9381, -0.4184,
                                     -0.5730, -0.4184, 1.0228), nrow = 3, ncol = 3, byrow = TRUE)

#set up the mu
mu <- c(1.1141, -0.6768, 3.3521)
    
# c sequence (from -1 to 1 by 0.1)
c_value <- seq(from = -1, to = 1, by = 0.1)
    
# Number of simulation rounds
R <- 1000
    
# Parameters 
# the n is for the data generate process within the loop
# the n_value of for the loop to keep going.
n_value <- c(100, 250, 500, 1000)
    
#the t-test with 0.05 degree of freedom (do not change to a you will lost it)
storage_matrix <- matrix(0, nrow = length(n_value), ncol = 3)
    
for (k in 1:length(n_value)){ 
       pval_matrix <- matrix(0, nrow = R, ncol = 2)
      #rest the n
      n <- n_value[k]

  for (i in 1:R){
  #data generate process
  simulated_data <- mvrnorm(n, mu, evil_variance_matrix)
        
  #The following code are specific for the model 0.4.
  simulated_data <- mvrnorm(n, mu, evil_variance_matrix)
  #note: here the data does not have the intercept
  # beta1 + beta 2 + beta 3
        
  #take the beta1, beta2 and beta3 out and repute them together. 
  #and I can repose it for the rest of the question 3!
  beta_one <- simulated_data[, 1]
  beta_two <- simulated_data[, 2]
  beta_three <- simulated_data[, 3]
        
  #the model 0.4 have intercept, beta_2 and beta_3
  x <- cbind(rep(1, n), simulated_data[, 2], simulated_data[, 3])
        
  e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
  #now its the time for the calculate the y!
  beta_very_true <- c(-0.8, 0, 0.1)  # As a vector 
        
  y <- x %*% beta_very_true + e
  # OLS estimation
  beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)
  eps_hat <- y - x %*% beta_hat
        
  #the degree of freedom 
  dof <- n - length(beta_hat)
  #the mu
  sigma_hat_sq <- (1 / dof) * sum(eps_hat^2)
  #estimate of the 
  v_beta_hat <- solve(t(x) %*% x) * sigma_hat_sq
  # standard error time!
  diag_v_beta_hat <- diag(v_beta_hat)
  sd <- sqrt(diag_v_beta_hat)
        
  #the T- test time!
  #T statistic for the beta2
  t_st <- (beta_hat[3,])/sd[3]
 #T test P value 
 pval_matrix[i, 1] <- 1 - pt(abs(t_st), dof) #using the TA's note!
  #the matrix time!
  #count the p_value
        
        # Compute and save the p-values
        
 ##################################
        

 #The model 0.5

 #take the beta1, beta2 and beta3 out and repute them together. 
 #and I can repose it for the rest of the question 3!
        
x <- cbind(rep(1, n), simulated_data[, 2])
        
e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
#now its the time for the calculate the y!
beta_very_true <- c(-0.8, 0)  # As a vector 
        
y <- x %*% beta_very_true + e
        
# OLS estimation
beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)
eps_hat <- y - x %*% beta_hat
        
#the degree of freedom 
dof <- n - length(beta_hat)
#the mu
sigma_hat_sq <- (1 / dof) * sum(eps_hat^2)
#estimate of the 
v_beta_hat <- solve(t(x) %*% x) * sigma_hat_sq
# standard error time!
diag_v_beta_hat <- diag(v_beta_hat)
sd <- sqrt(diag_v_beta_hat)
        
#the T- test time!
#T statistic for the beta2
t_st <- (beta_hat[2,])/sd[2]
#T test P value 
pval_matrix[i, 2] <- 1 - pt(abs(t_st), dof) #using the TA's note!
#the matrix time!
#count the p_value
       }
      
      
      # After completing R simulations, calculate the probabilities
      joint_probability <- sum(pval_matrix[, 1] > 0.05 & pval_matrix[, 2] < 0.05) / R
      probability_reject_0_4 <- sum(pval_matrix[, 1] < 0.05) / R
      probability_reject_0_5 <- sum(pval_matrix[, 2] < 0.05) / R
      
      # Store these probabilities in storage_matrix
      storage_matrix[k, 1] <- joint_probability
      storage_matrix[k, 2] <- probability_reject_0_4
      storage_matrix[k, 3] <- probability_reject_0_5
      
      colnames(storage_matrix) <- c("Joint Prob (Not Reject 0.4 AND Reject 0.5)",
                                    "Prob Reject 0.4",
                                    "Prob Reject 0.5")
      
      # Set row names based on n_value to make it more readable
      rownames(storage_matrix) <- paste("n =", n_value)
    }
    
    print(storage_matrix)

```

With the increase of the sample size \( n \), the joint probability of failing to reject the null hypothesis for model (0.4) while rejecting it for model (0.5) has decreased. This trend suggests that as we have more data, our tests become more accurate in distinguishing between the correctly specified model and the one that might be misspecified. The increasing sample size leads to more precise estimates, reducing the likelihood of making incorrect inferences about the population parameters.




Q3-D
I will using the example of Q3-A and A3-B to aruge with my boss that including an eredundant explanatory variable within the regression model will only increase the posibility of the type one error with a decreasing proference of the model. 


Q3-E

The significance of the main variable \(M_i\) might change after we remove a less important variable \(W_i\). This can happen for a few reasons that we should carefully think about it. First, \(W_i\) could be a control variable. Removing it might make \(M_i\) seem more important than it really is. Also, just because \(W_i\) doesn't seem important at first doesn't mean it has no value in explaining \(M_i\). It's possible that \(W_i\) and the outcome have a non-linear relationship that our model i doesn't pick up. To better understand how variables like \(M_i\) and \(W_i\) relate, we might want to try other methods like best subset selection or use lasso or ridge regression to decide if a variable should be included.


Q4-A

Q4-A-I

\[
\text{Use the law of iterated expectations}
\]

\begin{align*} 
E[\epsilon_i x_i] &= E[E[\epsilon_i x_i | x_i]] \\
&= E[E[\epsilon_i | x_i] \cdot x_i] \\
&= E[\delta \cdot x_i] \\
\end{align*}

If \( \delta = 0 \), then: 

\begin{align*}
E[\delta \cdot x_i] &= E[0 \cdot x_i] \\
                    &= 0
\end{align*}
$\therefore$ if \( E[\epsilon_i x_i] = 0 \), then \( E[\delta \cdot x_i] = 0 \). For the exception to be zero.
$\therefore$ we can concluded that \[ E[\epsilon_i x_i] = 0 \quad \text{if and only if} \quad \delta = 0\]

Q4-A-ii
We start with the law of iterated expectations.

If \( \delta \neq 0 \), then:
\begin{align*}
E[\epsilon_i x_i] &= E_x[E[\epsilon_i x_i | x_i]] \\
                  &= E[E[\epsilon_i | x_i] \cdot x_i] \\
                  &= E[\delta \cdot x_i]
\end{align*}

Therefore, \( E[\epsilon_i | x_i] \) is not zero if \( \delta \) is not zero, menans that:
\[
E[\epsilon_i | x_i] \neq 0
\]


4-a
```{r, echo=FALSE }
#clean the enviroment for thies question
rm(list = ls())
library(MASS)

set.seed(10086)

#some early process
#loading the important value
n <- 10000
B <- 999

#vector of mean
mu <- c(-0.22, 0.41, -1.56, 1.13, 0.82)

#covariance matrix
covariance_matrix <- matrix(
                  c(1.00,   0.00,  0.00,   0.250, -0.250,
                    0.00,   1.00,  0.00,  -0.125,  0.125,
                    0.00,   0.00,  1.00,   0.500,  0.000,
                    0.25, -0.125, 0.500,   1.000, -0.125,
                   -0.25,  0.125, 0.000,  -0.125,  1.000), 
                nrow = 5,
                 ncol = 5, 
                 byrow = TRUE)
    
#Draw data
x <- mvrnorm(n, mu, Sigma = covariance_matrix)
#Beta_ture
beta_true <- c(0.94, -0.24, 0.18, 1.17, -1.54, 0.72)

#Add intercept (we now turn the x from 3x3 matrix to 3x4 matrix)
x <- cbind(rep(1, n), x)
# e need to be processed within the loop 
e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
#now its the time for the calculate the y!
y <- x %*% beta_true + e


#compute the beta_hat
beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)


    #Matrix to store the boots sampling
boots_matriex <- matrix(ncol = 1 , nrow = B)

#The for loop time!
for (b in 1:B) {
  boot_sample <- sample(n, n, replace = TRUE)
  
  #resample the x and y
  x_boot <- x[boot_sample, ]
  y_boot <- y[boot_sample]
  
  #compute the beta_hat
  beta_hat_boot <- solve(t(x_boot) %*% x_boot) %*% (t(x_boot) %*% y_boot)
  
  #store the value
  boots_matriex[b] <- beta_hat_boot[2]
}

#plot the grpahy!
hist(boots_matriex, main = "Bootstrap Distribution of Beta_1", xlab = "Beta_1 Estimates", breaks = 50)

```

Q-4-B
```{R, echo=FALSE}

#clean the enviroment for thies question
rm(list = ls())
library(MASS)

set.seed(10086)

#some early process
#loading the important value
n <- 10000
B <- 999

#mu
mu <- c(-0.22, 0.41, -1.56, 1.13, 0.82)

#covariance matrix
covariance_matrix <- matrix(
                  c(1.00,   0.00,  0.00,   0.250, -0.250,
                    0.00,   1.00,  0.00,  -0.125,  0.125,
                    0.00,   0.00,  1.00,   0.500,  0.000,
                    0.25, -0.125, 0.500,   1.000, -0.125,
                   -0.25,  0.125, 0.000,  -0.125,  1.000), 
                nrow = 5,
                 ncol = 5, 
                 byrow = TRUE)
    
#Draw data
x <- mvrnorm(n, mu, Sigma = covariance_matrix)
#Beta_ture
beta_true <- c(0.94, -0.24, 0.18, 1.17, -1.54, 0.72)

#Add intercept (we now turn the x from 3x3 matrix to 3x4 matrix)
x <- cbind(rep(1, n), x)
# e need to be processed within the loop 
e <- rnorm(n, mean = 0, sd = 1)  #normal distribution 
#now its the time for the calculate the y!
y <- x %*% beta_true + e


#compute the beta_hat
beta_hat <- solve(t(x) %*% x) %*% (t(x) %*% y)


    #Matrix to store the boots sampling
boots_matriex <- matrix(ncol = 1 , nrow = B)

#The for loop time!
for (b in 1:B) {
  boot_sample <- sample(n, n, replace = TRUE)
  
  #resample the x and y
  x_boot <- x[boot_sample,]
  y_boot <- y[boot_sample]
  
  #compute the beta_hat
  beta_hat_boot <- solve(t(x_boot) %*% x_boot) %*% (t(x_boot) %*% y_boot)
  
  #store the value
  boots_matriex[b] <- beta_hat_boot[2]
}

#calculate the precentile of hteboots strap estimates.
q_25 <- quantile(boots_matriex, probs = 0.025)
q_975 <- quantile(boots_matriex, probs = 0.975)

#
#confidence interval
Q4B_bootstrap_confi_interval <- c(q_25, q_975)

#calculate the standard error
std_error <- sd(boots_matriex)


Q4B_theoretical_confi_interval <- beta_hat[2] + c(-1.96, 1.96) * std_error

#put together
confidence_intervals_matrix <- rbind(Q4B_bootstrap_confi_interval, Q4B_theoretical_confi_interval)
#rename the matirx
rownames(confidence_intervals_matrix) <- c("Bootstrap CI", "Theoretical CI")
#print the matrix 
print(confidence_intervals_matrix)

```

The output suggests that the confidence intervals are close between the Bootstrap and theoretical methods, meaning there is a robust estimate for beta_one in the linear regression model. Furthermore, it means that the true value is likely within the shared interval.

